{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScrapping: Crawler.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "HynXt4IEQ-MA",
        "5p_Kgyquq1Sy",
        "eLOGTKeRHafO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNITntni88BTbRwNzMLw7Da",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Echo9k/WebScrapping/blob/main/WebScrapping_Crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynXt4IEQ-MA"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6qvC-Vt767y",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install libraries\n",
        "%%capture\n",
        "!rm sample_data -r\n",
        "!pip install fake-useragent\n",
        "!pip install fastprogress\n",
        "# !pip install w3lib\n",
        "# !pip install selenium\n",
        "# !apt-get update # to update ubuntu to correctly run apt install\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXc81jeLtPTa",
        "cellView": "form"
      },
      "source": [
        "#@markdown Importing dependencies\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# HTML\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from requests.exceptions import HTTPError\n",
        "from IPython.core.display import display, HTML\n",
        "from urllib.parse import unquote\n",
        "# import mechanize\n",
        "\n",
        "# Selenium for JS support\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "# from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCxOZ4LqeBs"
      },
      "source": [
        "JavaScript\r\n",
        "Try first soup.decode [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\r\n",
        "\r\n",
        "If you need to try something else go with Phantom or headless_driver (slower).\r\n",
        "```\r\n",
        "def get_attribute(id, attribute):\r\n",
        "    return headles_driver.find_element_by_id(id).get_attribute(attribute)\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETiYZEh-bmy",
        "cellView": "form"
      },
      "source": [
        "#@markdown As: headles_driver\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# #@markdown As: headles_driver\n",
        "# headles_driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "#@markdown As: phantom_driver\n",
        "# from selenium import webdriver\n",
        "# !wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !cp phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin\n",
        "# !ls -al\n",
        "# phantom_driver = webdriver.PhantomJS()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYDUfFPbZmdn",
        "cellView": "form"
      },
      "source": [
        "#@title Functions\n",
        "## Lambdas\n",
        "render = lambda html_contents: display(HTML(html_contents))\n",
        "\n",
        "# @markdown * search_tag\n",
        "def search_tag(tag, string):\n",
        "    regex=r\"(?:\"+tag+\"=)\"\n",
        "    if tag in string:\n",
        "        split_1 = re.split(regex,string)[1].replace('%20', ' ')\n",
        "        print(f\"{tag}: found in text\\n\"\n",
        "            f\"contains:{split_1}\")\n",
        "# @markdown * regex_finder\n",
        "def regex_finder(regex:str, text:str,*,\n",
        "           look_before:int=10,\n",
        "           look_ahead:int=250,\n",
        "           extra_dots=1) -> str:\n",
        "    \"\"\"\n",
        "    # RETURNS: group found, match\n",
        "    \"\"\"\n",
        "    matches = re.finditer(regex, text, re.MULTILINE | re.IGNORECASE | re.UNICODE)\n",
        "    \n",
        "    for matchNum, match in enumerate(matches, start=1):\n",
        "        print(f\"Match: {matchNum} {match.group()}\")\n",
        "        # , match = match.group()\n",
        "        \n",
        "        for groupNum in range(0, len(match.groups())):\n",
        "            groupNum = groupNum + 1\n",
        "            return match.group()[:-extra_dots], match.string[match.start(groupNum)-look_before:match.end(groupNum)+look_ahead]\n",
        "\n",
        "# @markdown ### Tags:\n",
        "# @markdown The RegEx strings are stored in the directory: _labels_ <br> <br>\n",
        "regex_labels= {\n",
        "    \"Brand\":r\"(?:brand|brandname|vendor|manufacturer|product-brand)(?![&])(.)\",\n",
        "    \"Manufacturer\":r\"(?:manufacturer|mfr|mfg|manufacturer logo|manufacturer name|label|producer|fabricante|fabrikant|Hersteller)(?![&])(.)\",\n",
        "    \"Category\":r\"(?:category|categories|category path|breadcrumbs|breadcrum|crumb|navbar|Product Category)(?![&])(.)\",\n",
        "    \"Model\":r\"(?:sku|model|model id|model no|item number|itemid|article no|product number|style number|product id|item code|mfr no|data-product)(?![&])(.)\",\n",
        "    \"UPC\":r'(?:\"UPC\"|\"GTIN\"|\"EAN\"|\"upc\"|\"upccode\"|\"product_upc\"|\"product:upc\"|\"gtin\"|\"ean\"|\"barcode\")',\n",
        "    \"Part\":r\"(?:PN|P/N|part no|part number|part|part #|mpn)(?![&g])(...)\",\n",
        "    \"Color\":r\"(?:color|color_name|shade|finish|shade description)(?![&])(.)\",\n",
        "    \"size\":r\"(?:selected size|available size|choose a size|product size|attribute pa size)(?![&])(.)\",\n",
        "    \"list_price\":r\"(?:MSRP|MRP|Recommended Customer Price|USD MSRP|List Price|reseller price may vary)(?![&])(.)\",\n",
        "    \"item_count\":r\"(?:count|pieces|ct|pc|combo|per pack|contains)(?![&])(.)\",\n",
        "    \"IPQ\":r\"(?:packs|packs of|pk|package|combo|carton|carton pack)(?![&])(.)\",\n",
        "    \"product_description\":r\"(?:Product Details|Specification|Tech specs|Technical specification|Details|see more features|Product Description|Description|About the product|ingredients|Where to use|How to use)(?![&])(.)\"\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_Kgyquq1Sy"
      },
      "source": [
        "# Explore tags\r\n",
        "One option we have to obtain the product detaill's from a URL would be using the labels corresponding and attribute to find their necesary value. We can use Regular Expressions (RegEx) to speed up this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ptfYPBnlRA9",
        "cellView": "form"
      },
      "source": [
        "#@title Read URL\r\n",
        "URL = \"https://www.myblisskiss.com/dropper-squeeze-pen-combo-kit/?Pack=3%20Pack&Scent=Crisp\" #@param {type:\"string\"}\r\n",
        "show = False #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "# Retrieve URL\r\n",
        "if len(URL)>0:\r\n",
        "    response = requests.request('GET', URL,  headers={'User-Agent': 'Mozilla/5.0'})\r\n",
        "    soup = bs(response.text)\r\n",
        "    pretty_soup = soup.prettify()\r\n",
        "    if show:\r\n",
        "        render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL8qOSsKl70d",
        "cellView": "form"
      },
      "source": [
        "find = \"UPC\" #@param [\"Brand\", \"Manufacturer\", \"Category\", \"Model\", \"UPC\", \"Part\", \"Color\", \"size\", \"list_price\", \"item_count\", \"IPQ\", \"product_description\"]\r\n",
        "look_ahead = 45 #@param {type:\"slider\", min:0, max:100, step:5}\r\n",
        "try: display(regex_finder(regex_labels.get(find), soup.text, look_ahead=look_ahead))\r\n",
        "except: \"something happened\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G9119S8g3a2",
        "cellView": "form"
      },
      "source": [
        "#@title By id and attribute\n",
        "\n",
        "tag_type = \"div\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Attributes\n",
        "attributes = {} #@param {type:\"raw\"}\n",
        "_class = \"woocommerce-product-details__short-description\" #@param {type:\"string\"}\n",
        "_id = \"\" #@param {type:\"string\"}\n",
        "_itemprop = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if len(_class)>0: attributes.update({\"class\":_class})\n",
        "if len(_id)>0: attributes.update({\"id\":_id})\n",
        "if len(_itemprop)>0: attributes.update({\"itemprop\":_itemprop})\n",
        "\n",
        "split_text = None #@param {type:\"raw\"}\n",
        "strip_ = True #@param{type:\"boolean\"}\n",
        "\n",
        "find_all = False #@param {type:\"boolean\"}\n",
        "try:\n",
        "    if find_all:\n",
        "        test = soup.find_all(tag_type, attributes)\n",
        "        formula = f\"page.soup.find_all('{tag_type}', {attributes})\"\n",
        "        test = [t.text for t in test]\n",
        "        print(f\"len {len(test)}\\n\", test.get_text())\n",
        "    else:\n",
        "        test = soup.find(tag_type, attributes)\n",
        "        formula = f\"clean_it(page.soup.find('{tag_type}', {attributes}))\"\n",
        "        display(clean_it(test, split_text=split_text, strip_=strip_))\n",
        "    display(f\"{formula}\")\n",
        "except AttributeError: raise\n",
        "except: print(page.url); raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZ-XSDN1upf"
      },
      "source": [
        "# Search\r\n",
        "A different paradigm is to use a list of URL to retrive and save their soups at once, thus reducing server work and reducing processing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAg7udQZnziR"
      },
      "source": [
        "## Additional setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ruNqf-fQg9b",
        "cellView": "form"
      },
      "source": [
        "#@markdown Additional libraries\n",
        "import json\n",
        "import time\n",
        "import unicodedata\n",
        "from fake_useragent import UserAgent\n",
        "from google.colab import files\n",
        "from fastprogress import master_bar, progress_bar\n",
        "# from w3lib.html import replace_entities\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from google.colab import data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNzqzKpSDY0k",
        "cellView": "form"
      },
      "source": [
        "#@markdown Functions\r\n",
        "# Labmdas\r\n",
        "def eprint(*args, **kwargs):\r\n",
        "    print(*args, file=sys.stderr, **kwargs)\r\n",
        "\r\n",
        "lap_time = lambda start_time: time.time() - start_time\r\n",
        "def __variant_extractor(squeezed_url):\r\n",
        "    try: return int(squeezed_url)\r\n",
        "    except TypeError: return None\r\n",
        "    except: return squeezed_url\r\n",
        "\r\n",
        "def __url_info(url):\r\n",
        "    return_ = {}\r\n",
        "    try:\r\n",
        "        url_info_asis = url.split('?',1)[1]\r\n",
        "        details = url_info_asis.split('&')\r\n",
        "        for i in details:\r\n",
        "            split = i.split('=')\r\n",
        "            return_.update({split[0]:split[1]})\r\n",
        "    except IndexError: return_ = {\"empty\":None}\r\n",
        "    except AttributeError: print(f\"AttributeError {url}\"); return_ = None\r\n",
        "    except: raise\r\n",
        "    return return_\r\n",
        "\r\n",
        "def clean_it(text, *,form='NFKC', split_text = \"\\n\" , strip_=True):\r\n",
        "    if split_text is None: \r\n",
        "        text = text.get_text(strip=strip_)\r\n",
        "    else:\r\n",
        "        text= text.get_text(split_text, strip=strip_)\r\n",
        "    if form is not None: return unicodedata.normalize(form, text)\r\n",
        "    else: return text\r\n",
        "# Retrieve data\r\n",
        "def load_file(file_name, kwargs):\r\n",
        "    def __read_csv(file_name, kwargs):\r\n",
        "        try: return pd.read_csv(file_name, **kwargs)\r\n",
        "        except: display(\"Invalid arguments (CSV)\"); raise\r\n",
        "\r\n",
        "    def __read_excel(file_name, kwargs):\r\n",
        "        try: return pd.read_csv(file_name, **kwargs)\r\n",
        "        except: display(\"Invalid kargs (Excel)\"); raise\r\n",
        "\r\n",
        "    # read\r\n",
        "    if file_name.rsplit('.',1)[-1] in ['csv','text']:\r\n",
        "        print('format: csv/text')\r\n",
        "        return __read_csv(file_name, kwargs)\r\n",
        "    elif file_name.rsplit('.',1)[-1] in ['xls','xlsx']:\r\n",
        "        print('format: Excel')\r\n",
        "        return __read_excel(file_name, kwargs)\r\n",
        "    else: \"unknown format\"\r\n",
        "\r\n",
        "## Column names\r\n",
        "# all_columns=[\"ID\",\"website\",\"title\",\"bread_crumb1\",\"bread_crumb2\",\"bread_crumb3\",\"brand_name\",\r\n",
        "#              \"manufacturer_name\",\"model\",\"upc\",\"color_name\",\"size_name\",\r\n",
        "#              \"item_package_quantity\",\"part_number\",\"list_price\",\"unit_count\",\r\n",
        "#              \"product_description\"]\r\n",
        "\r\n",
        "usual_columns=[\"url\",\"title\",\"bread_crumb1\",\"bread_crumb2\",\"bread_crumb3\",\r\n",
        "            \"brand_name\", \"manufacturer_name\",\"model\",\"upc\",\r\n",
        "             \"color_name\",\"size_name\", \"item_package_quantity\",\"unit_count\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHLaDDJ_C5SJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Class: RetrivePage<br>\n",
        "#@markdown If \"Damnit, unicode\" is guessing the encoder wrong\n",
        "exclude_encodings = None #@param [\"None\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "\n",
        "\n",
        "class RetrivePage():\n",
        "    \"\"\"This is a page class meant to retrive a page and return it's soup object\"\"\"\n",
        "\n",
        "    def __init__(self, url, headless=False, phantom=False):\n",
        "        def __variant_extractor(url):\n",
        "            squeezed_url = np.squeeze(re.findall(r\"=(.*)\", x))\n",
        "            try: return int(squeezed_url)\n",
        "            except TypeError: return None\n",
        "            except: return squeezed_url\n",
        "        self.url = url\n",
        "        self.response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        self.soup = bs(self.response.text, exclude_encodings=exclude_encodings)\n",
        "        self.parsed_url = unquote(url)\n",
        "        self.variant = __variant_extractor(self.url)\n",
        "        self.metadata = {}\n",
        "        self.variant_info = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tFLhrZBn2ye"
      },
      "source": [
        "## Workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaO4JwhyDnJM",
        "cellView": "form"
      },
      "source": [
        "#@title Get the Websites\r\n",
        "\r\n",
        "file_name = \"/content/Pinkiou.csv\" #@param {type:\"string\"}\r\n",
        "kwargs = {\"usecols\": usual_columns} #@param [\"{\\\"usecols\\\": usual_columns}\", \"{\\\"usecols\\\": [\\\"url\\\"]}\", \"{}\"] {type:\"raw\", allow-input: true}\r\n",
        "#@markdown Select **user agent**\r\n",
        "# User agent to workaround sites that don't allow bots\r\n",
        "header = \"Chrome\" #@param [\"Chrome\", \"Edge\", \"Random\", \"None\"]\r\n",
        "usr_agent = UserAgent()\r\n",
        "\r\n",
        "def user_agent_retrive(i):\r\n",
        "    switcher={\r\n",
        "            \"Edge\":usr_agent.edge,\r\n",
        "            \"Chrome\":usr_agent.chrome,\r\n",
        "            \"Random\":usr_agent.random\r\n",
        "            }\r\n",
        "    return switcher.get(i,None)\r\n",
        "header = user_agent_retrive(header)\r\n",
        "#@markdown _Looking for more options?_  [fake_useragent](https://pypi.python.org/pypi/fake-useragent)\r\n",
        "def load_URLs(URL):\r\n",
        "    r = requests.request('GET', URL, headers={'User-Agent': usr_agent})\r\n",
        "    return bs(r.text), phantom_driver.get(URL), headles_driver.get(URL)\r\n",
        "\r\n",
        "#@markdown Data stored as: data\r\n",
        "data = load_file(file_name, kwargs)\r\n",
        "\r\n",
        "## separating URLs\r\n",
        "url_variant = data.url[data.url.apply(lambda x: \"=\" in x)]\r\n",
        "not_variant = data.url[data.url.apply(lambda x: \"=\" not in x)]\r\n",
        "\r\n",
        "print(f\"Not variant URLs: {len(not_variant)}\\n\"\r\n",
        "      f\"URL with variant: {len(url_variant)}\")\r\n",
        "\r\n",
        "#@markdown Info from the URL: **url_info**\r\n",
        "url_infos = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in data.url.apply(__url_info).items() ])).T\r\n",
        "url_infos=url_infos.applymap(lambda x:unquote(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCVUpFS4GVjz",
        "cellView": "form"
      },
      "source": [
        "#@markdown ...loading<br>\n",
        "#@markdown Create a new pd.Series to store the web scrapped RetrivePage objects.<br>\n",
        "start_time = time.time()\n",
        "pages = pd.Series([RetrivePage(url) for url in progress_bar(data.url)])\n",
        "lap_time(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zCmS9GjD08",
        "cellView": "form"
      },
      "source": [
        "#@title Find index of a given URL\r\n",
        "_get_url = lambda x:x.url\r\n",
        "urls = pages.apply(_get_url)\r\n",
        "urls_dict = {url:i for i,url in enumerate(urls)}\r\n",
        "url = \"https://www.pinkiou.com/product/pinkiou-eyebrow-tattoo-needles-for-permanent-makeup-penmicroblading-pen/?size=tattoo%20needles&color=7%20Pin\" #@param {type:\"string\"}\r\n",
        "urls_dict.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWOTxSOT2mZn",
        "cellView": "form"
      },
      "source": [
        "#@markdown Retrieve a page from it's index\n",
        "\n",
        "index =    105#@param {type:\"integer\"}\n",
        "page = pages[index]\n",
        "soup = page.soup\n",
        "url = page.url\n",
        "variant = page.variant\n",
        "example = {'index':index,\n",
        "           'page':page,\n",
        "           'soup':soup,\n",
        "           'url':url,\n",
        "           'variant':variant}\n",
        "print(f\"url: {page.url}\")\n",
        "show = False #@param {type:\"boolean\"}\n",
        "if show:\n",
        "    render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3ijGWRIrjei"
      },
      "source": [
        "## Test this first\n",
        "These are functions that have worked well in the past.\n",
        "Give them a try before experimenting something new."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upbBodR83G0u",
        "cellView": "form"
      },
      "source": [
        "#@markdown #### Regex test*\r\n",
        "pattern = '(\\d+-)*\\d+mm'\r\n",
        "pattern = re.compile(pattern)\r\n",
        "\r\n",
        "extracts = [re.search(pattern, str(page)) for page in data.title]\r\n",
        "def _ex(e):\r\n",
        "    try: return e.group()\r\n",
        "    except: return None\r\n",
        "pd.Series([_ex(e) for e in extracts]).to_csv(\"extracts.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLOGTKeRHafO"
      },
      "source": [
        "### From scripts\r\n",
        "Often pages relay on a data structure for keeping the information of the different variants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4gX5PvKnBO",
        "cellView": "form"
      },
      "source": [
        "#@title Simple script meta\n",
        "attributes = {\"data-desc\":\"seo-product\", \"type\":\"application/ld+json\"} #@param{trype:\"raw\"}\n",
        "key_name = \"script_meta\" #@param {type:\"string\"}\n",
        "def __script_meta(page):\n",
        "    script_meta = soup.find('script', attributes).text\n",
        "    json_data = json.loads(script_meta)\n",
        "    try: page.metadata.update({key_name:json_data})\n",
        "    except: page.script_meta = None\n",
        "    return page.metadata[key_name]\n",
        "apply_to_all = True #@param{type:\"boolean\"}\n",
        "if apply_to_all: pages.apply(__script_meta)\n",
        "__script_meta(page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eqNZml4NnIr",
        "cellView": "form"
      },
      "source": [
        "# @title Metadata\r\n",
        "# @markdown This is useful when you have a tag such as **var meta =**\r\n",
        "var_name = \"var __st=\" #@param {'type':'string'}\r\n",
        "key_name = \"__st\" #@param {'type':'string'}\r\n",
        "if key_name is None:\r\n",
        "    key_name = var_name.strip().split()[1]\r\n",
        "get_variant_info = False #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "print(f\"key_name: {key_name}\")\r\n",
        "def __get_variant_info(page):\r\n",
        "    # try:\r\n",
        "    #     variants = page.metadata['product']['variants']\r\n",
        "    #     return [i for i in variants if int(i['id']) == int(page.variant)][0]\r\n",
        "    # except: None\r\n",
        "    try: return [v for v in page.metadata[key_name]['product']['variants'] if int(v['id']) == int(page.variant)][0]\r\n",
        "    except: None\r\n",
        "\r\n",
        "def get_meta(page):\r\n",
        "    soup = page.soup\r\n",
        "    scripts = soup.find_all('script')\r\n",
        "    # mets = ''\r\n",
        "    for s in scripts:\r\n",
        "        if var_name in s.text:          # find the script of interest\r\n",
        "            # Exctract the json value of var_name\r\n",
        "            meta = s.text\r\n",
        "            meta = meta.split(var_name)[1].split('};')[0]+'}'\r\n",
        "            # Load the json and make's sure the format is correct\r\n",
        "            json_meta = json.loads(meta)\r\n",
        "            page.metadata.update({key_name:json_meta}) # add metadata to the RetrivedPage\r\n",
        "            if get_variant_info:\r\n",
        "                page.variant_info.update({key_name:__get_variant_info(page)})\r\n",
        "            return json_meta\r\n",
        "json_meta = pages.apply(get_meta)\r\n",
        "display(get_meta(page))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9bKWDdBzrfG",
        "cellView": "form"
      },
      "source": [
        "#@title Script metadata\r\n",
        "#@markdown For when the metadata comes from a script tag\r\n",
        "identifier=\"bold-platform-data\" #@param {type:\"string\"}\r\n",
        "left_delimiter = '}' #@param {type:\"string\"}\r\n",
        "script_idx  =  0#@param {'type':'integer'}\r\n",
        "#@markdown Save as:\r\n",
        "script_key = \"script_meta\" #@param {'type':'string'}\r\n",
        "def __script_metadata(page):\r\n",
        "    def __get_variant_info(page):\r\n",
        "        try: return [v for v in page.metadata[script_key]['product']['variants'] if int(v['id']) == int(page.variant)][0]\r\n",
        "        except: None\r\n",
        "    \r\n",
        "    def _clean_script(identifier, left_delimiter=None, script_idx=0):\r\n",
        "        script = [i for i in page.soup.find_all('script') if identifier in str(i)][script_idx]\r\n",
        "        script_clean = '{'+str(script).split('{',1)[1].split(left_delimiter,1)[0]+left_delimiter+'}}'\r\n",
        "        try: return json.loads(script_clean)\r\n",
        "        except json.JSONDecodeError:\r\n",
        "            print(f\"JSONDecodeError {page.url}\")\r\n",
        "            return script_clean\r\n",
        "\r\n",
        "    \r\n",
        "    try:\r\n",
        "        script_metadata = _clean_script(identifier, left_delimiter, script_idx)\r\n",
        "        page.metadata.update({script_key:script_metadata})\r\n",
        "        try: page.variant_info.update({script_key:__get_variant_info(page)})\r\n",
        "        except: pass\r\n",
        "        return script_metadata\r\n",
        "    except:\r\n",
        "        return None\r\n",
        "print(f\"script_key: {script_key}\")\r\n",
        "script_metadatas = pages.apply(__script_metadata)\r\n",
        "script_metadatas[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QP31uP6FhpP",
        "cellView": "form"
      },
      "source": [
        "#@title By attribute\n",
        "#@markdown Find items by their attributes\n",
        "\n",
        "tag_type = \"div\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Attributes\n",
        "attributes = {} #@param {type:\"raw\"}\n",
        "_class = \"\" #@param {type:\"string\"}\n",
        "_id = \"breadcrumbs\" #@param {type:\"string\"}\n",
        "_itemprop = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if len(_class)>0: attributes.update({\"class\":_class})\n",
        "if len(_id)>0: attributes.update({\"id\":_id})\n",
        "if len(_itemprop)>0: attributes.update({\"itemprop\":_itemprop})\n",
        "\n",
        "split_text =  None#@param {type:\"raw\"}\n",
        "strip_ = True #@param{type:\"boolean\"}\n",
        "\n",
        "find_all = False #@param {type:\"boolean\"}\n",
        "try:\n",
        "    if find_all:\n",
        "        test = soup.find_all(tag_type, attributes)\n",
        "        formula = f\"page.soup.find_all('{tag_type}', {attributes})\"\n",
        "        test = [t.text for t in test]\n",
        "        print(f\"len {len(test)}\\n\", test.get_text())\n",
        "    else:\n",
        "        test = soup.find(tag_type, attributes)\n",
        "        formula = f\"clean_it(page.soup.find('{tag_type}', {attributes}))\"\n",
        "        display(clean_it(test, split_text=split_text, strip_=strip_))\n",
        "    display(f\"{formula}\")\n",
        "except AttributeError: raise\n",
        "except: print(page.url); raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Db8nua0mRv"
      },
      "source": [
        "### Useful functions to start with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9KSqxzr2dlQ",
        "cellView": "form"
      },
      "source": [
        "#@title titles\n",
        "def _title(page):\n",
        "    try: page.title = clean_it(page.soup.find('h1', {'class': 'product_title entry-title'}))\n",
        "    except: page.title = None\n",
        "    return page.title\n",
        "\n",
        "titles = pages.apply(__title)\n",
        "print(titles[index])\n",
        "titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vKYHRtvER0x",
        "cellView": "form"
      },
      "source": [
        "#@title Crumbs\r\n",
        "def __get_crumbs(page):\r\n",
        "    result = {\"crumb_0\":None,\"crumb_1\":None,\"crumb_2\":None}\r\n",
        "    try:\r\n",
        "        # Just change this one\r\n",
        "        crumbs =  [i.strip() for i in clean_it(page.soup.find('div', {'id': 'breadcrumbs'})).split('/')]\r\n",
        "        if crumbs[0] != page.title:\r\n",
        "            try: result[\"crumb_0\"]=crumbs[0]\r\n",
        "            except: pass\r\n",
        "            if crumbs[1] != page.title:\r\n",
        "                try: result[\"crumb_1\"]=crumbs[1]\r\n",
        "                except: pass\r\n",
        "                if crumbs[2] != page.title:\r\n",
        "                    try: result[\"crumb_2\"]=crumbs[2]\r\n",
        "                    except: pass\r\n",
        "    except AttributeError: print(page.url)\r\n",
        "    except: raise\r\n",
        "    finally: return result\r\n",
        "crumbs = pages.apply(__get_crumbs)\r\n",
        "\r\n",
        "bread_crumb1 = pd.Series([i['crumb_0'] for i in crumbs])\r\n",
        "bread_crumb2 = pd.Series([i['crumb_1'] for i in crumbs])\r\n",
        "bread_crumb3 = pd.Series([i['crumb_2'] for i in crumbs])\r\n",
        "\r\n",
        "display(crumbs[index])\r\n",
        "\r\n",
        "print(\"unique values:\\n\"\r\n",
        "    f\"• bread_crumb1: {bread_crumb1.nunique()}\\n\"\r\n",
        "    f\"• bread_crumb2: {bread_crumb2.nunique()}\\n\"\r\n",
        "    f\"• bread_crumb3: {bread_crumb3.nunique()}\\n\"\r\n",
        "    )\r\n",
        "display(crumbs.sample(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIaB8D0cF5lu"
      },
      "source": [
        "#  Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVPsZJcPD8AR"
      },
      "source": [
        "### Work in progress experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGl5D6swZmw"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dry_cvNmHErA",
        "cellView": "form"
      },
      "source": [
        "#@markdown Result\r\n",
        "result = {\"url\":urls}\r\n",
        "result.update({\r\n",
        "    \"title\":titles,\r\n",
        "    \"bread_crumb1\":breadcrumb1,\r\n",
        "    \"bread_crumb2\":breadcrumb2,\r\n",
        "    \"bread_crumb3\":breadcrumb3,\r\n",
        "    # \"brand_name\":brands,\r\n",
        "    # 'manufacturer_name':manufacturer_names,\r\n",
        "    # \"product_description\":descriptions,\r\n",
        "    # \"model\":skus,\r\n",
        "    # \"upc\":barcodes,\r\n",
        "    # \"part_number\":MPNs,\r\n",
        "    # \"color_name\":colors,\r\n",
        "    # \"size_name\":sizes,\r\n",
        "    # \"item_package_quantity\":IPQs\r\n",
        "    # \"unit_count\":unit_counts\r\n",
        "\r\n",
        "    # Temporarl keys\r\n",
        "    # \"sku_upc\":\r\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBt-fL60HlRl",
        "cellView": "form"
      },
      "source": [
        "#@markdown Progress\r\n",
        "show_progress = True #@param {type:\"boolean\"}\r\n",
        "df_result = pd.DataFrame(result)\r\n",
        "if show_progress:\r\n",
        "    display(data_table.DataTable(df_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "148CcoNQCk9_",
        "cellView": "form"
      },
      "source": [
        "\n",
        "df_result = pd.DataFrame(result)\n",
        "export = \"reviewed\" #@param [\"reviewed\", \"joined\"]\n",
        "fill_na_with = \"___BLANK_CELL___\" #@param  {type:\"string\"}\n",
        "auto_download = True #@param {type:\"boolean\"}\n",
        "\n",
        "_export_name = file_name.split('.')[0] + f\"_{export}.xlsx\"\n",
        "_result_column_set = set(df_result.columns)\n",
        "_data_column_set = set(data.columns)\n",
        "_intersection = _data_column_set.intersection(_result_column_set)\n",
        "_intersection.discard('url') # inplace opperation\n",
        "print(\"Intersection\", _intersection)\n",
        "\n",
        "df_join = data.drop(columns=_intersection)\n",
        "df_join = pd.concat([df_join.set_index('url'), df_result.set_index('url')],axis=1,)\\\n",
        "                .fillna(fill_na_with).replace('',fill_na_with)\n",
        "\n",
        "_df_join_columns=set(df_join.columns)\n",
        "_df_join_columns.discard('url')\n",
        "\n",
        "if export == \"joined\":\n",
        "    try:\n",
        "        df_join = df_join.reindex(columns=list(_df_join_columns))\n",
        "        print(f\"df_join \\t[reindexed]\\t{time.time()}\")\n",
        "        if auto_download: files.download(_export_name)\n",
        "    except:\n",
        "        print(f\"df_join \\t[couldn't reindex]\\t{time.time()}\")\n",
        "        display(_df_join_columns.symmetric_difference(_data_column_set))\n",
        "    df_join.to_excel(_export_name)\n",
        "    if auto_download: files.download(_export_name)\n",
        "else:\n",
        "    df_result.fillna(fill_na_with)\\\n",
        "            .replace('',fill_na_with)\\\n",
        "            .to_excel(_export_name)\n",
        "    if auto_download: files.download(_export_name)\n",
        "    print(f\"df_result\\t{time.time()}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}