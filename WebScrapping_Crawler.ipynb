{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScrapping: Crawler.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "HynXt4IEQ-MA",
        "5p_Kgyquq1Sy",
        "eLOGTKeRHafO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdHpfs+hviVcJnq6sxGji1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Echo9k/WebScrapping/blob/main/WebScrapping_Crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynXt4IEQ-MA"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6qvC-Vt767y",
        "cellView": "form"
      },
      "source": [
        "#@title Install libraries\n",
        "#@markdown use only in colab [Dismissed temporarly]\n",
        "!rm sample_data -r\n",
        "# !pip install w3lib\n",
        "# !pip install selenium\n",
        "# !apt-get update # to update ubuntu to correctly run apt install\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXc81jeLtPTa"
      },
      "source": [
        "#@title Set up\n",
        "#@markdown Loading dependencies...\n",
        "import os\n",
        "import re\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# HTML\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from requests.exceptions import HTTPError\n",
        "from IPython.core.display import display, HTML\n",
        "from urllib.parse import unquote\n",
        "# import mechanize\n",
        "\n",
        "# Selenium for JS support\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "# from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETiYZEh-bmy",
        "cellView": "form"
      },
      "source": [
        "#@title Headless\n",
        "#@markdown As: headles_driver\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# #@markdown As: headles_driver\n",
        "# headles_driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6pHvgAXaVh",
        "cellView": "form"
      },
      "source": [
        "#@title PhantomJS\n",
        "#@markdown As: phantom_driver\n",
        "# from selenium import webdriver\n",
        "# !wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !cp phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin\n",
        "# !ls -al\n",
        "# phantom_driver = webdriver.PhantomJS()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_Kgyquq1Sy"
      },
      "source": [
        "# Explore with tags\r\n",
        "One option we have to obtain the product detaill's from a URL would be using the labels corresponding and attribute to find their necesary value. We can use Regular Expressions (RegEx) to speed up this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZTECalQFfSW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Regex strings defined under: *variable*_regex\n",
        "brand_regex = r\"(?:brand|brandname|vendor|manufacturer|product-brand)(?![&])(.)\"\n",
        "crumb_regex = r\"(?:category|categories|category path|breadcrumbs|breadcrum|crumb|navbar|Product Category)(?![&])(.)\"\n",
        "sku_regex = r\"(?:sku|model|model id|model no|item number|itemid|article no|product number|style number|product id|item code|mfr no|data-product)(?![&])(.)\"\n",
        "model_regex = '(?:sku|model|model id|model no|item number|itemid|article no|product number|style number|product id|item code|mfr no|data-product)(?![&])(.)'\n",
        "upc_regex = r'(?:\"UPC\"|\"GTIN\"|\"EAN\"|\"upc\"|\"upccode\"|\"product_upc\"|\"product:upc\"|\"gtin\"|\"ean\"|\"barcode\")'\n",
        "part_regex = r\"(?:PN|P/N|part no|part number|part|part #|mpn)(?![&g])(...)\"\n",
        "color_regex = r\"(?:color|color_name|shade|finish|shade description)(?![&])(.)\"\n",
        "size_regex = r\"(?:selected size|available size|choose a size|product size|attribute pa size)(?![&])(.)\"\n",
        "mfr_regex = r\"(?:manufacturer|mfr|mfg|manufacturer logo|manufacturer name|label|producer|fabricante|fabrikant|Hersteller)(?![&])(.)\"\n",
        "price_regex = r\"(?:MSRP|MRP|Recommended Customer Price|USD MSRP|List Price|reseller price may vary)(?![&])(.)\"\n",
        "ct_regex = r\"(?:count|pieces|ct|pc|combo|per pack|contains)(?![&])(.)\"\n",
        "pk_regex = r\"(?:packs|packs of|pk|package|combo|carton|carton pack)(?![&])(.)\"\n",
        "description_regex = r\"(?:Product Details|Specification|Tech specs|Technical specification|Details|see more features|Product Description|Description|About the product|ingredients|Where to use|How to use)(?![&])(.)\"\n",
        "#@markdown The RegEx strings are stored in the directory: _labels_ <br> <br>\n",
        "labels= {\n",
        "    \"Brand Name\":brand_regex,\n",
        "    \"Category Name\":crumb_regex,\n",
        "    \"SKU\":sku_regex,\n",
        "    \"Model Name\":model_regex,\n",
        "    \"UPC\":upc_regex,\n",
        "    \"Part Number\":part_regex,\n",
        "    \"Color name\":color_regex,\n",
        "    \"Size Name\":size_regex,\n",
        "    \"Manufacturer Name\":mfr_regex,\n",
        "    \"List Price\":price_regex,\n",
        "    \"Item Count\":ct_regex,\n",
        "    \"Item Package Quantity\":pk_regex,\n",
        "    \"Product Description\":description_regex\n",
        "    }\n",
        "\n",
        "#@markdown ### Functions\n",
        "#@markdown * Search tag\n",
        "#@markdown * Finder\n",
        "def search_tag(tag, string):\n",
        "    regex=r\"(?:\"+tag+\"=)\"\n",
        "    if tag in string:\n",
        "        split_1 = re.split(regex,string)[1].replace('%20', ' ')\n",
        "        print(f\"{tag}: found in text\\n\"\n",
        "            f\"contains:{split_1}\")\n",
        "\n",
        "def finder(regex:str, text:str,*,\n",
        "           look_before:int=10,\n",
        "           look_ahead:int=250,\n",
        "           extra_dots=1) -> str:\n",
        "    \"\"\"\n",
        "    # RETURNS: group found, match\n",
        "    \"\"\"\n",
        "    matches = re.finditer(regex, text, re.MULTILINE | re.IGNORECASE | re.UNICODE)\n",
        "    \n",
        "    for matchNum, match in enumerate(matches, start=1):\n",
        "        print(f\"Match: {matchNum} {match.group()}\")\n",
        "        # , match = match.group()\n",
        "        \n",
        "        for groupNum in range(0, len(match.groups())):\n",
        "            groupNum = groupNum + 1\n",
        "            return match.group()[:-extra_dots], match.string[match.start(groupNum)-look_before:match.end(groupNum)+look_ahead]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ptfYPBnlRA9",
        "cellView": "form"
      },
      "source": [
        "#@title Read URL\r\n",
        "# Functions\r\n",
        "render = lambda html_contents: display(HTML(html_contents))\r\n",
        "\r\n",
        "# Attributes\r\n",
        "web_driver = False #@param {type:\"boolean\"}\r\n",
        "URL = \"https://www.fultonperformance.com/products.aspx/trailer-accessories/trailer-fenders/trailer-fender/WV5KcOu1jGUOChEob70bwOTxorgeYPz1iJvSEqIJ0V0%3d\" #@param {type:\"string\"}\r\n",
        "show = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "# Retrieve URL\r\n",
        "if len(URL)>0:\r\n",
        "    response = requests.request('GET', URL)\r\n",
        "    soup = bs(response.text)\r\n",
        "    pretty_soup = soup.prettify()\r\n",
        "\r\n",
        "    if web_driver:\r\n",
        "        wd.get(URL)\r\n",
        "\r\n",
        "    if show:\r\n",
        "        render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lL8qOSsKl70d"
      },
      "source": [
        "try:\r\n",
        "    find = \"Manufacturer Name\" #@param ['Brand Name', 'Category Name', 'Model Name', 'UPC', 'Part Number', 'Color name', 'Size Name', 'Manufacturer Name', 'List Price', 'Item Count', 'Item Package Quantity', 'Product Description']\r\n",
        "    finder(labels.get(find), soup.text, look_ahead=10)\r\n",
        "except:\r\n",
        "    \"something happened\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eFvkSVc-l-Ha"
      },
      "source": [
        "#@title By id and attribute\r\n",
        "try:\r\n",
        "    id = \"ProductPrice\" #@param {type:\"string\"}\r\n",
        "    attribute = \"itemprop\" #@param {type:\"string\"}\r\n",
        "    try:\r\n",
        "        wd.find_element_by_id(id).get_attribute(attribute=attribute)\r\n",
        "    except:\r\n",
        "        print(\"something happened\")\r\n",
        "except:\r\n",
        "    \"something happened\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCxOZ4LqeBs"
      },
      "source": [
        "Most of this can be done through the libraries: soup, and BeautifulSoup4.\r\n",
        "\r\n",
        "For sites which rely heavly on JS it can also be usefull to use Phantom or headless_driver to access specific attributes.\r\n",
        "```\r\n",
        "def get_attribute(id, attribute):\r\n",
        "    return headles_driver.find_element_by_id(id).get_attribute(attribute)\r\n",
        "```\r\n",
        "These two last options are slower to load and often you can find a workaround using soup.decode.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZ-XSDN1upf"
      },
      "source": [
        "# Recursive search\r\n",
        "A different paradigm is to use a list of URL to retrive and save their soups at once, thus reducing server work and reducing processing time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ruNqf-fQg9b",
        "cellView": "form"
      },
      "source": [
        "#@title Additional set up\n",
        "\n",
        "#@markdown * Import libraries\n",
        "import json\n",
        "import time\n",
        "import unicodedata\n",
        "# from w3lib.html import replace_entities\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from google.colab import data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHLaDDJ_C5SJ",
        "cellView": "form"
      },
      "source": [
        "#@title Introducing a Class: RetrivePage\n",
        "\n",
        "class RetrivePage:\n",
        "    \"This is a page class meant to retrive a page and return it's soup object\"\n",
        "    def __init__(self, url, headless=False, phantom=False):\n",
        "        __variant_extractor_lambda = lambda x: np.squeeze(re.findall(r\"=(.*)\", x))\n",
        "        self.url = url\n",
        "        self.soup = bs(requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}).text)\n",
        "        self.parsed_url = unquote(url)\n",
        "        self.headless_driver = None\n",
        "        self.phantom_driver = None\n",
        "        self.variant = __variant_extractor_lambda(self.url)\n",
        "        if headless:\n",
        "            self.headless_driver = get_headless_driver()\n",
        "        if phantom:\n",
        "            self.phantom_driver = get_phantom_driver()\n",
        "    \n",
        "    def apply(self, function):\n",
        "        return function(self.page)\n",
        "    \n",
        "    # # Headless driver\n",
        "    # def get_headless_driver(self):\n",
        "    #     return headles_driver.get(self.url)\n",
        "    # def build_headless(self):\n",
        "    #     self.headless_driver = self.get_headless_driver()\n",
        "\n",
        "    # # Phantom driver\n",
        "    # def build_phantom(self):\n",
        "    #     self.phantom_driver = self.get_phantom_driver()\n",
        "    # def get_phantom_driver(self):\n",
        "    #     return phantom_driver.get(self.url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNzqzKpSDY0k",
        "cellView": "form"
      },
      "source": [
        "#@title Functions\r\n",
        "#@markdown * Retriving data\r\n",
        "#Functions\r\n",
        "def load_file(file_name, usecols):\r\n",
        "    def __read_csv(file_name, usecols=useful_cols, **kwargs):\r\n",
        "        try:\r\n",
        "            return pd.read_csv(file_name, usecols)\r\n",
        "        except:\r\n",
        "            display(\"Dismissed: useful_cols\")\r\n",
        "            return pd.read_csv(file_name)\r\n",
        "\r\n",
        "    def __read_excel(file_name, usecols=useful_cols, **kwargs):\r\n",
        "        try:\r\n",
        "            return pd.read_csv(file_name, usecols)\r\n",
        "        except:\r\n",
        "            display(\"Dismissed: useful_cols\")\r\n",
        "\r\n",
        "            return pd.read_excel(file_name)\r\n",
        "    # read\r\n",
        "    if file_name.rsplit('.',1)[-1] in ['csv','text']:\r\n",
        "        print('text')\r\n",
        "        return __read_csv(file_name, usecols=useful_cols, **kwargs)\r\n",
        "    elif file_name.rsplit('.',1)[-1] in ['xls','xlsx']:\r\n",
        "        print('Excel')\r\n",
        "        return __read_excel(file_name, usecols=useful_cols, **kwargs)\r\n",
        "    else:\r\n",
        "        \"unknown format\"\r\n",
        "#@markdown List of useful columns\r\n",
        "#[\"url\",\"brand_name\",\"manufacturer_name\",\"product_description\",\"model\",\"color_name\",\"item_package_quantity\", \"unit_count\"]\r\n",
        "all_columns=[\"title\",\"bread_crumb1\",\"bread_crumb2\",\"bread_crumb3\",\"brand_name\",\r\n",
        "             \"manufacturer_name\",\"model\",\"upc\",\"color_name\",\"size_name\",\r\n",
        "             \"item_package_quantity\",\"part_number\",\"list_price\",\"unit_count\",\r\n",
        "             \"product_description\"]\r\n",
        "useful_cols =  [\"url\",\"brand_name\",\"manufacturer_name\",\"product_description\",\"model\",\"color_name\",\"item_package_quantity\", \"unit_count\"]#@param {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaO4JwhyDnJM",
        "cellView": "form"
      },
      "source": [
        "#@title Get the Websites\r\n",
        "wait_time =  0 #@param {type:\"integer\"}\r\n",
        "def load_URLs(URL):\r\n",
        "    r = requests.request('GET', URL)\r\n",
        "    time.sleep(wait_time)\r\n",
        "    return bs(r.text), phantom_driver.get(URL), headles_driver.get(URL)\r\n",
        "\r\n",
        "file_name = \"/content/nakamichicaraudio.csv\" #@param {type:\"string\"}\r\n",
        "kwargs  =  {} #@param {type:\"raw\"}\r\n",
        "#@markdown Data loaded as: data\r\n",
        "data = load_file(file_name, usecols=useful_cols)\r\n",
        "\r\n",
        "## separating URLs\r\n",
        "url_variant = data.url[data.url.apply(lambda x: \"=\" in x)]\r\n",
        "not_variant = data.url[data.url.apply(lambda x: \"=\" not in x)]\r\n",
        "\r\n",
        "print(f\"Not variant URLs: {len(not_variant)}\\n\"\r\n",
        "      f\"URL with variant: {len(url_variant)}\")\r\n",
        "\r\n",
        "#@markdown Create a new pd.Series to store the web scrapped RetrivePage objects.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVgu2VkwMMoN"
      },
      "source": [
        "#@markdown ...loading\r\n",
        "start_time = time.time()\r\n",
        "stopwatch = lambda x: time.time() - start_time\r\n",
        "\r\n",
        "pages_1 = data[:40].url.apply(RetrivePage), print(\"20%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_2 = data[40:80].url.apply(RetrivePage), print(\"40%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_3 = data[80:120].url.apply(RetrivePage), print(\"60%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_4 = data[120:160].url.apply(RetrivePage), print(\"80%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_5 = data[160:].url.apply(RetrivePage), print(\"100%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages = pd.concat([pages_1[0], pages_2[0], pages_3[0],pages_4[0], pages_5[0]])\r\n",
        "print(\"Completed%\", \"time: \", stopwatch(start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaf4USSXi_v1",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### **Start** â–º URL & title\r\n",
        "_get_url = lambda x:x.url\r\n",
        "urls = pages.apply(_get_url)\r\n",
        "result = {\"url\":urls} #@param {type:\"raw\"}\r\n",
        "#@markdown include titles?\r\n",
        "boolean = False #@param {'type':'boolean'}\r\n",
        "if boolean:\r\n",
        "    _get_title = lambda x:x.soup.title.text.strip('\\n').rstrip('\\n')\r\n",
        "    titles = pages.apply(_get_title)\r\n",
        "    result.update({\"title\":titles})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBt-fL60HlRl"
      },
      "source": [
        "#@markdown Progress\r\n",
        "show_progress = True #@param {type:\"boolean\"}\r\n",
        "df_result = pd.DataFrame(result)\r\n",
        "if show_progress:\r\n",
        "    display(data_table.DataTable(df_result))\r\n",
        "\r\n",
        "export_file = False #@param {type:\"boolean\"}\r\n",
        "__export_name = file_name.split('.')[0] + \"_reviwed.csv\"\r\n",
        "if export_file:\r\n",
        "    df_result.to_csv(__export_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zCmS9GjD08",
        "cellView": "form"
      },
      "source": [
        "#@markdown Find url' index\r\n",
        "urls_dict = {url:i for i,url in enumerate(urls)}\r\n",
        "url = \"https://nakamichicaraudio.com/products/nakamichi-na-md1?variant=29234205032501\" #@param {type:\"string\"}\r\n",
        "urls_dict.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWOTxSOT2mZn",
        "cellView": "form"
      },
      "source": [
        "#@markdown Example page: example\n",
        "index =   12 #@param {type:\"integer\"}\n",
        "page = pages[index]\n",
        "soup = page.soup\n",
        "url = page.url\n",
        "variant = page.variant\n",
        "example = {'index':index,\n",
        "           'page':page,\n",
        "           'soup':soup,\n",
        "           'url':url,\n",
        "           'variant':variant}\n",
        "print(f\"url: {page.url}\")\n",
        "show = False #@param {type:\"boolean\"}\n",
        "if show:\n",
        "    render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83C_nC6vLIT"
      },
      "source": [
        "## Finding additional attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmQJcdEjYrRA"
      },
      "source": [
        "### Done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUsX1sm-tWnJ"
      },
      "source": [
        "result.update({\"title\":titles})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NitKnWnfiXxC"
      },
      "source": [
        "# variants = pages.apply(lambda x: x.variant)\r\n",
        "parsed_urls = pages.apply(lambda page:np.squeeze((page.parsed_url.split('?')[1:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7APGQX10qbDp"
      },
      "source": [
        "pattern = re.compile(r\"\\d+\")\r\n",
        "def __get_uc(parsed_url):\r\n",
        "    try: return re.findall(pattern, parsed_url)\r\n",
        "    except: return parsed_url\r\n",
        "unit_counts = parsed_urls.apply(__get_uc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GlwjiCehoY5o"
      },
      "source": [
        "#@markdown Breadcrumbs\r\n",
        "def __get_crumbs(page):\r\n",
        "    crumb = page.soup.find(\"nav\", {\"class\":\"woocommerce-breadcrumb\"})\r\n",
        "    try:\r\n",
        "        return unicodedata.normalize(\"NFKD\", crumb.text)\r\n",
        "    except:\r\n",
        "        return crumb\r\n",
        "crumbs = pages.apply(__get_crumbs)\r\n",
        "result.update({\"crumbs\":crumbs})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMQDo3hzolX3"
      },
      "source": [
        "#@markdown Titles\r\n",
        "def __get_titles(page):\r\n",
        "    title = page.soup.find(\"h1\", {\"class\":\"product_title entry-title\"})\r\n",
        "    try:\r\n",
        "        return unicodedata.normalize(\"NFKD\", title.text)\r\n",
        "    except:\r\n",
        "        return title\r\n",
        "titles = pages.apply(__get_titles)\r\n",
        "result.update({\"titles\":titles})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI0RsjcMHHi5"
      },
      "source": [
        "# text = page.soup.find(\"div\", {'class':\"woocommerce-tabs wc-tabs-wrapper\"}).text\r\n",
        "unicodedata.normalize('NFKC', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elg_1J0kHgC5"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t\t\t\t\t\tDescription\t\t\t\t\t\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t\t\t\t\t\tDirections\t\t\t\t\t\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t\t\t\t\t\tReviews (5)\t\t\t\t\t\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t\t\t\t\t\tPartner Assets\t\t\t\t\t\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Yard Odor Eliminator Plus Citronella Spray is for your lawn and yard. Do not spray directly on pets.\r\n",
        "Yard Odor Eliminator Plus Citronella helps eliminate stool and urine odors. Spray on any outdoor surface. For use on grass, plants, shrubs, patios, patio furniture, kennels, dog runs, swing sets, fences, block walls or any other surface where odors arise due to pets. If spraying on fabric, test product on a very small inconspicuous surface area before using.\r\n",
        "Caution:\r\n",
        "Keep out of the reach of children. Do not spray directly on pets. Avoid contact with eyes or accidental ingestion. Keep children and pets from sprayed from sprayed area until dry. In case of allergic reaction or accidental ingestion consult a health professional immediately. Do not spray around fish ponds or where run-off will drain into ponds. To avoid run-off or puddling, do not over-spray product.\r\n",
        "\r\n",
        "\r\n",
        "Directions:\r\n",
        "Shake well before using. Connect sprayer to garden hose. Turn on water. To begin spraying, point nozzle in the direction you want to spray. Turn plastic knob to â€œONâ€ position. Spray evenly over area. To stop spraying turn knob to â€œOFFâ€ position. Turn off water and disconnect sprayer from hose. Yard Odor Eliminator Plus quickly eliminates pet odors from your yard due to stool and urine, leaving your yard with a pleasant citronella scent. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t\t\t5 reviews for Yard Odor Eliminator Plus Citronella Spray \r\n",
        "Disclaimer: These testimonials are for informational purposes only. The information is not a substitute for expert veterinary care. Testimonials are written by actual customers and represent their own observations. These observations are not guaranteed, are not medically substantiated, and may not be typical for other pets.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Rated 5 out of 5\r\n",
        "\r\n",
        "Angus Tillson \r\n",
        "â€“ May 26, 2016\r\n",
        "\r\n",
        "This product helped get rid of those â€œdogâ€ smells that were always wafting inside our house from the backyard. Now I get a nice citronella smell. Sure beats the odors I was smelling! Great product, it does what it says ðŸ™‚\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Rated 5 out of 5\r\n",
        "\r\n",
        "Cindy \r\n",
        "â€“ July 16, 2016\r\n",
        "\r\n",
        "Does what it says but smell is only gone about a week. Still a bottle does 3 doses for our area and thatâ€™s great. Also keeps the mosquitoes and insects away from the house a bit too. Smells great, citronella smell is not really strong. Definitely reordering regularly.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Jenifer L Jordan \r\n",
        "â€“ September 2, 2019\r\n",
        "\r\n",
        "What are the ingredients?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "NaturVet \r\n",
        "â€“ September 4, 2019\r\n",
        "\r\n",
        "Primarily deionized water, and citronella oil.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Christine \r\n",
        "â€“ September 1, 2020\r\n",
        "\r\n",
        "How often do I need to reapply?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "NaturVet \r\n",
        "â€“ September 2, 2020\r\n",
        "\r\n",
        "Hi Christine.  Thank you for your product question.  The Yard Odor Eliminator will eliminate the odors instantly however they will return once the area is re-contaminated.  You can use the product as many times as necessary.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Matt \r\n",
        "â€“ September 20, 2020\r\n",
        "\r\n",
        "Can the Yard Odor Eliminator Refill be used in a hand-pump sprayer?   I donâ€™t have a water source near the area that needs spraying, so the hose hookup bottle wonâ€™t work for me.  Does it need to be diluted with water...and if so, what is the ratio?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "NaturVet \r\n",
        "â€“ September 21, 2020\r\n",
        "\r\n",
        "Hi Matt.  Thank you for your product question.  You can dilute the product 50/50 to apply it from a hand-pump sprayer.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Add a review Cancel replyYour email address will not be published. Required fields are marked *Your rating *\r\n",
        "Rate...\r\n",
        "Perfect\r\n",
        "Good\r\n",
        "Average\r\n",
        "Not that bad\r\n",
        "Very poor\r\n",
        "Your review *Name *\r\n",
        "Email *\r\n",
        " Save my name, email, and website in this browser for the next time I comment.\r\n",
        " \r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "[partner-assets]\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsMdEwB4TJvu"
      },
      "source": [
        "### Work In Progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLOGTKeRHafO"
      },
      "source": [
        "#### From scripts\r\n",
        "Often pages relay on a data structure for keeping the information of the different variants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sOcRv3r3N4Z",
        "cellView": "form"
      },
      "source": [
        "# @title Metadata\r\n",
        "# @markdown **var_name** From where we're going to obtain the values.\r\n",
        "var_name = \"SUBParams = \" #@param {'type':'string'}\r\n",
        "def get_meta(page):\r\n",
        "    soup = page.soup\r\n",
        "    scripts = soup.find_all('script')\r\n",
        "    mets = ''\r\n",
        "    for s in scripts:\r\n",
        "        if var_name in s.text:          # find the script of interest\r\n",
        "            # Exctract the json value of var_name\r\n",
        "            meta = s.text\r\n",
        "            meta = meta.split(var_name)[1].split('};')[0]+'}'\r\n",
        "            # Load the json and make's sure the format is correct\r\n",
        "            json_meta = json.loads(meta)\r\n",
        "            page.SUBParams = json_meta # add metadata to the RetrivedPage\r\n",
        "            return json_meta\r\n",
        "    print(count)\r\n",
        "metadata = pages.apply(get_meta)\r\n",
        "# @markdown **result_key** Name to store the results in the result' directory.\r\n",
        "result_key = \"metadata\" #@param {type:\"string\"}\r\n",
        "result.update({result_key:metadata}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "V-at6B4ZKtIe"
      },
      "source": [
        "#@markdown vendor\r\n",
        "_get_vendor = lambda m: m['product']['vendor']\r\n",
        "vendor = metadata.apply(_get_vendor)\r\n",
        "result.update({\"vendor\":vendor})\r\n",
        "\r\n",
        "#@markdown resourceId\r\n",
        "_get_resourceID = lambda m: m['page']['resourceId']\r\n",
        "resourceId = metadata.apply(_get_resourceID)\r\n",
        "result.update({\"resourceId\":resourceId})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbzcY2lHkGDU"
      },
      "source": [
        "#@title Variant Info\r\n",
        "def __variant_info(page):\r\n",
        "    ps= page.SUBParams\r\n",
        "\r\n",
        "    var_ls = ps['product']['variants']\r\n",
        "    if len(var_ls)>1:\r\n",
        "        for v in var_ls:\r\n",
        "            s=''\r\n",
        "            try: s = str(i['id'])\r\n",
        "            except: page.variant=None\r\n",
        "            if s == page.variant:\r\n",
        "                page.vSUBParams = [i for i in var_ls if i['id'] == page.variant][0]\r\n",
        "            else:\r\n",
        "                page.vSUBParams = None\r\n",
        "    else:\r\n",
        "        page.vSUBParams = var_ls[0]\r\n",
        "\r\n",
        "    try:return page.vSUBParams\r\n",
        "    except:return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIxYa6vEskXM"
      },
      "source": [
        "variant_info = pages.apply(__variant_info)\r\n",
        "def __get_barcode(b):\r\n",
        "    try: return str(b['barcode'])\r\n",
        "    except: return b\r\n",
        "barcodes = barcode.apply(__get_barcode)\r\n",
        "result.update({\"barcodes\":barcodes})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYQCbvvbMQmw"
      },
      "source": [
        "# parsed_urls = pages.apply(lambda page: page.parsed_url)\r\n",
        "pattern = r\"\\dpc\"\r\n",
        "[i for parsed_url in parsed_urls for i in re.finditer(pattern, parsed_url, re.MULTILINE)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS5F_kCLsuYx"
      },
      "source": [
        "#@markdown Progress: df\r\n",
        "show_sample = True #@param {type:\"boolean\"}\r\n",
        "df = pd.DataFrame(result)\r\n",
        "df.drop(\"metadata\", 1, inplace=True)\r\n",
        "if show_sample:\r\n",
        "    display(data_table.DataTable(df))\r\n",
        "\r\n",
        "df.resourceId = resourceId.apply(lambda x: str(x))\r\n",
        "df.to_excel(\"Rukket.xlsx\",index=\"urls\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YJzGPJdxVvC2"
      },
      "source": [
        "#@title Description\r\n",
        "def __get_descriptions(page):\r\n",
        "    soup = page.soup\r\n",
        "    content = soup.find_all('div', {\"class\":\"TabbedPanelsContent\"})\r\n",
        "    description_tabs = soup.find(\"ul\", attrs={\"class\":\"TabbedPanelsTabGroup\"})\r\n",
        "    description_tabs = description_tabs.text\\\r\n",
        "                                    .rstrip('\\n').strip('\\n')\\\r\n",
        "                                    .split('\\n')\r\n",
        "\r\n",
        "    description = ''\r\n",
        "    ignore = ['Optional Accessories', 'Impeller Kits']\r\n",
        "\r\n",
        "    def clean_text(text_to_clean):\r\n",
        "        return text_to_clean.strip('\\n').rstrip('\\n').strip('\\xa0').rstrip('\\n').strip('\\n')\r\n",
        "\r\n",
        "    def __post_process_description(description):\r\n",
        "        description_noWarranty = ''\r\n",
        "        for i, strr in enumerate(description.split('\\n')):\r\n",
        "            if (len(strr) < 15) & (\"arranty\" in strr):\r\n",
        "                ++i\r\n",
        "            else:\r\n",
        "                description_noWarranty += strr\r\n",
        "        return description_noWarranty\\\r\n",
        "                            .rstrip('\\n')\\\r\n",
        "                            .replace('Ã¢\\x80Â¢', 'â€¢')\\\r\n",
        "                            .replace('Ã‚Â°', 'Â°')\\\r\n",
        "                            .replace('Ã¢Â€Â™s',\"'s\")\r\n",
        "                            \r\n",
        "\r\n",
        "    try:\r\n",
        "        for i, tab_name in enumerate(description_tabs):\r\n",
        "            if tab_name not in ignore:\r\n",
        "                info = clean_text(content[i].text)\r\n",
        "                description += tab_name +  '\\n' + info + '\\n'\r\n",
        "        return __post_process_description(description)\r\n",
        "    except IndexError:\r\n",
        "        print(page.url)\r\n",
        "descriptions = pages.apply(__get_descriptions)\r\n",
        "# df = df.join(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7o0Q1EK6SI"
      },
      "source": [
        "### Done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzWrU5G0Yyoi"
      },
      "source": [
        "### json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn2tytkg_FVZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Find Attributes\n",
        "dc = json.loads(finder(r'(?:var meta = )(.*)(?:};)', s.text)[0].replace('var meta = ',''))\n",
        "try:\n",
        "    variant_num = URL.split('variant=')[1]\n",
        "    variant = [i for i in dc['product']['variants'] if i['id']==int(variant_num)][0]\n",
        "except IndexError:\n",
        "    variant = dc['product']['variants'][0]\n",
        "#@markdown * title\n",
        "title = driver.find_element_by_class_name(\"standard-single\").text\n",
        "\n",
        "#@markdown * Public_title  | _has the size values_\n",
        "public_title = variant['public_title'] \n",
        "\n",
        "#@markdown * brand\n",
        "brand = finder(r\"(?:\\\"brand\\\")(.)\", s.text, look_ahead=20)[1].split(\":\")[1].replace('\"','')\n",
        "\n",
        "#@markdown * manofacturer\n",
        "try:\n",
        "    label = dc['product']['label']\n",
        "except:\n",
        "    label = finder(r\"(?:\\\"label\\\")(.)\", s.text, look_ahead=20)[1].split(\":\")[1].replace('\"','')\n",
        "\n",
        "#@markdown * Price\n",
        "price = wd.find_element_by_id('ProductPrice')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZvKuWVpjZ--"
      },
      "source": [
        "# Crawler\r\n",
        "Finally, another paradigm is defining a the page structure to obtain the values using a crawler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cJnl98h-ZB2S"
      },
      "source": [
        "#@title Imports\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ39WtoMSoRs"
      },
      "source": [
        "#@title Legacy sites\r\n",
        "import json\r\n",
        "formats_json = json.dumps([\r\n",
        "   \"demeterfragrance.com\",\r\n",
        "   {\r\n",
        "      \"Title\":\"title\",\r\n",
        "      \"BarcodeTag\":\"sku\",\r\n",
        "      \"Description\":\"<div>\"\r\n",
        "   }\r\n",
        "])\r\n",
        "legacy_sites = json.loads(formats_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRfLRfKpQZhF"
      },
      "source": [
        "# Key- Retrival function\r\n",
        "kwargs = {'title':(lambda p: p.title)}\r\n",
        "\r\n",
        "# Class: Website \r\n",
        "class Website:\r\n",
        "    \"\"\"\r\n",
        "    Contains information about website structure.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, url, **kwargs):\r\n",
        "        self.url = url\r\n",
        "        self.__dict__.update(kwargs)\r\n",
        "\r\n",
        "# Initialize the website as an empty entity\r\n",
        "# kwargs_n = {keys:None for keys in kwargs.keys()}\r\n",
        "# w = Website(page.url, kwargs_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cz56pzXjY9G"
      },
      "source": [
        "class Crawler:\r\n",
        "    def __init__(self, attrs:[dir]):\r\n",
        "        self.attrs=attrs\r\n",
        "\r\n",
        "    def getPage(self, url):\r\n",
        "        try:\r\n",
        "            req = requests.get(url)\r\n",
        "        except requests.exceptions.RequestException:\r\n",
        "            return None\r\n",
        "        return BeautifulSoup(req.text, 'html.parser')\r\n",
        "\r\n",
        "    def safeGet(self, pageObj, selector):\r\n",
        "        \"\"\"\r\n",
        "        Utility function used to get a content string from a\r\n",
        "        Beautiful Soup object and a selector. Returns an empty\r\n",
        "        string if no object is found for the given selector\r\n",
        "        \"\"\"\r\n",
        "        selectedElems = pageObj.soup.select(selector)\r\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\r\n",
        "            return '\\n'.join([elem.get_text()\r\n",
        "                for elem in selectedElems])\r\n",
        "        return 'empty'\r\n",
        "\r\n",
        "    def ifer(self, page, sfun:[str, callable]):\r\n",
        "        if callable(sfun):\r\n",
        "            return sfun(page)\r\n",
        "        else:\r\n",
        "            return self.safeGet(page, sfun)\r\n",
        "\r\n",
        "    def parser(self, pageObj=None, url:[str]=None):\r\n",
        "        \"\"\"\r\n",
        "        Extract content from a given page URL\r\n",
        "        \"\"\"\r\n",
        "        if pageObj is None:\r\n",
        "            if url is not None:\r\n",
        "                pageObj = self.getPage(url)\r\n",
        "            else:\r\n",
        "                \"You need to pass one of pageObj/url\"\r\n",
        "        else:\r\n",
        "            url = pageObj.url\r\n",
        "\r\n",
        "        attrs = {key:self.ifer(pageObj, sfun)\r\n",
        "                for key, sfun in self.attrs.items()\r\n",
        "                }\r\n",
        "        return Website(pageObj.url, page=pageObj, **attrs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrqXWzoV3VQ5"
      },
      "source": [
        "# c = Crawler(kwargs)\r\n",
        "w=c.parser(page.soup, page.url)\r\n",
        "page.url"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}