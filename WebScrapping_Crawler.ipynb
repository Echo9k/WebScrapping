{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScrapping: Crawler.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "HynXt4IEQ-MA",
        "5p_Kgyquq1Sy",
        "eLOGTKeRHafO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoyq0wF7OKCgZ9K+1zj15x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Echo9k/WebScrapping/blob/main/WebScrapping_Crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynXt4IEQ-MA"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6qvC-Vt767y"
      },
      "source": [
        "#@title Install libraries\n",
        "#@markdown use only in colab [Dismissed temporarly]\n",
        "!rm sample_data -r\n",
        "!pip install fake-useragent\n",
        "# !pip install w3lib\n",
        "# !pip install selenium\n",
        "# !apt-get update # to update ubuntu to correctly run apt install\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXc81jeLtPTa"
      },
      "source": [
        "#@title Set up\n",
        "#@markdown Loading dependencies...\n",
        "import os\n",
        "import re\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# HTML\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from requests.exceptions import HTTPError\n",
        "from IPython.core.display import display, HTML\n",
        "from urllib.parse import unquote\n",
        "# import mechanize\n",
        "\n",
        "# Selenium for JS support\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "# from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETiYZEh-bmy",
        "cellView": "form"
      },
      "source": [
        "#@title Headless\n",
        "#@markdown As: headles_driver\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# #@markdown As: headles_driver\n",
        "# headles_driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6pHvgAXaVh",
        "cellView": "form"
      },
      "source": [
        "#@title PhantomJS\n",
        "#@markdown As: phantom_driver\n",
        "# from selenium import webdriver\n",
        "# !wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2\n",
        "# !cp phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin\n",
        "# !ls -al\n",
        "# phantom_driver = webdriver.PhantomJS()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_Kgyquq1Sy"
      },
      "source": [
        "# Explore with tags\r\n",
        "One option we have to obtain the product detaill's from a URL would be using the labels corresponding and attribute to find their necesary value. We can use Regular Expressions (RegEx) to speed up this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZTECalQFfSW"
      },
      "source": [
        "#@markdown Regex strings defined under: *variable*_regex\n",
        "brand_regex = r\"(?:brand|brandname|vendor|manufacturer|product-brand)(?![&])(.)\"\n",
        "crumb_regex = r\"(?:category|categories|category path|breadcrumbs|breadcrum|crumb|navbar|Product Category)(?![&])(.)\"\n",
        "sku_regex = r\"(?:sku|model|model id|model no|item number|itemid|article no|product number|style number|product id|item code|mfr no|data-product)(?![&])(.)\"\n",
        "model_regex = '(?:sku|model|model id|model no|item number|itemid|article no|product number|style number|product id|item code|mfr no|data-product)(?![&])(.)'\n",
        "upc_regex = r'(?:\"UPC\"|\"GTIN\"|\"EAN\"|\"upc\"|\"upccode\"|\"product_upc\"|\"product:upc\"|\"gtin\"|\"ean\"|\"barcode\")'\n",
        "part_regex = r\"(?:PN|P/N|part no|part number|part|part #|mpn)(?![&g])(...)\"\n",
        "color_regex = r\"(?:color|color_name|shade|finish|shade description)(?![&])(.)\"\n",
        "size_regex = r\"(?:selected size|available size|choose a size|product size|attribute pa size)(?![&])(.)\"\n",
        "mfr_regex = r\"(?:manufacturer|mfr|mfg|manufacturer logo|manufacturer name|label|producer|fabricante|fabrikant|Hersteller)(?![&])(.)\"\n",
        "price_regex = r\"(?:MSRP|MRP|Recommended Customer Price|USD MSRP|List Price|reseller price may vary)(?![&])(.)\"\n",
        "ct_regex = r\"(?:count|pieces|ct|pc|combo|per pack|contains)(?![&])(.)\"\n",
        "pk_regex = r\"(?:packs|packs of|pk|package|combo|carton|carton pack)(?![&])(.)\"\n",
        "description_regex = r\"(?:Product Details|Specification|Tech specs|Technical specification|Details|see more features|Product Description|Description|About the product|ingredients|Where to use|How to use)(?![&])(.)\"\n",
        "#@markdown The RegEx strings are stored in the directory: _labels_ <br> <br>\n",
        "labels= {\n",
        "    \"Brand Name\":brand_regex,\n",
        "    \"Category Name\":crumb_regex,\n",
        "    \"SKU\":sku_regex,\n",
        "    \"Model Name\":model_regex,\n",
        "    \"UPC\":upc_regex,\n",
        "    \"Part Number\":part_regex,\n",
        "    \"Color name\":color_regex,\n",
        "    \"Size Name\":size_regex,\n",
        "    \"Manufacturer Name\":mfr_regex,\n",
        "    \"List Price\":price_regex,\n",
        "    \"Item Count\":ct_regex,\n",
        "    \"Item Package Quantity\":pk_regex,\n",
        "    \"Product Description\":description_regex\n",
        "    }\n",
        "\n",
        "#@markdown ### Functions\n",
        "#@markdown * Search tag\n",
        "#@markdown * Finder\n",
        "def search_tag(tag, string):\n",
        "    regex=r\"(?:\"+tag+\"=)\"\n",
        "    if tag in string:\n",
        "        split_1 = re.split(regex,string)[1].replace('%20', ' ')\n",
        "        print(f\"{tag}: found in text\\n\"\n",
        "            f\"contains:{split_1}\")\n",
        "\n",
        "def finder(regex:str, text:str,*,\n",
        "           look_before:int=10,\n",
        "           look_ahead:int=250,\n",
        "           extra_dots=1) -> str:\n",
        "    \"\"\"\n",
        "    # RETURNS: group found, match\n",
        "    \"\"\"\n",
        "    matches = re.finditer(regex, text, re.MULTILINE | re.IGNORECASE | re.UNICODE)\n",
        "    \n",
        "    for matchNum, match in enumerate(matches, start=1):\n",
        "        print(f\"Match: {matchNum} {match.group()}\")\n",
        "        # , match = match.group()\n",
        "        \n",
        "        for groupNum in range(0, len(match.groups())):\n",
        "            groupNum = groupNum + 1\n",
        "            return match.group()[:-extra_dots], match.string[match.start(groupNum)-look_before:match.end(groupNum)+look_ahead]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ptfYPBnlRA9"
      },
      "source": [
        "#@title Read URL\r\n",
        "# Functions\r\n",
        "render = lambda html_contents: display(HTML(html_contents))\r\n",
        "\r\n",
        "# Attributes\r\n",
        "web_driver = False #@param {type:\"boolean\"}\r\n",
        "URL = \"http://www.kregtool.com/on/demandware.store/Sites-kreg-Site/en_US/Search-ShowAjax?cgid=shop&prefn1=badges&prefv1=awardWinning&prefn2=type&prefv2=Bench%20Clamps%20with%20Base%7cClamp%20Vises%7cTrack%20Clamp%7cClamp%20Table%7cDriver%20Bits%7cHardware%20Containers%7cProject%20Table%20Replacement%20Top%7cSwing%20Stop%7cRouter%20Table%20Fence%7cPlug%20Cutters%7cBeading%20Bit%7cScrew%20Kits%7cRouter%20Table%20Setup%20Bars%7cJig%20%26%20Fixture%20Bar%7cNotching%20Bit%7cDeck%20Screws%7cClamp%20Trak%20Kits%7cReplacement%20Guide%20Strips%7cCrown%20Molding%20Cutter%7cScrews%7cStepped%20Drill%20Bits%7cRail%20Sets%7cRouter%20Table%20Switch%7cVersa%20Stops%7cClamp%20Plates%7cClamp%20Traks%7cJackets%7cMiter%20Gauge%7cJig%20Expansion%20Pack%7cGuide%20Blocks%7cDeck%20Jig%7cMeasuring%20Tool%7cProject%20Table%20Tops%7cExpansion%20Pack%7cProject%20Table%20Kit%7cRight%20Angle%20Clamps%7cRouter%20Lift%7cDrawer%20Slide%20Jigs%7cPocket-Hole%20Machines%7cPlugs%7cCabinet%20Hardware%20Jigs%7cMaster%20Kit%7cDriver%20Combo%7cFace%20Clamps%7cClamp%20Table%20and%20Stand%20Combo%7cDrill%20Guide%20Spacer%20Block%7cTrack%20Horses%7cCombo%20Track%7cSaw%20%26%20Guide%20Track%20Kit%7cHats%7cPlunge%20Saw%7cBand%20Saw%20Fence%7cDust%20Collection%20Attachment%7cMicro%20Adjuster%7cPlate%20Levelers%7cFace-Framing%20Table%7cRouter%20Table%20Top%7cRouter%20Table%20System%7cHinge%20Jigs%7cSaw%20Track%20Combo%7cAdhesive%20Measuring%20Tape%7cDrill%20Bits%7cSystem%20Organizer%7cRip%20Guides\" #@param {type:\"string\"}\r\n",
        "show = False #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "# Retrieve URL\r\n",
        "if len(URL)>0:\r\n",
        "    response = requests.request('GET', URL)\r\n",
        "    soup = bs(response.text)\r\n",
        "    pretty_soup = soup.prettify()\r\n",
        "\r\n",
        "    if web_driver:\r\n",
        "        wd.get(URL)\r\n",
        "\r\n",
        "    if show:\r\n",
        "        render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lL8qOSsKl70d"
      },
      "source": [
        "try:\r\n",
        "    find = \"Manufacturer Name\" #@param ['Brand Name', 'Category Name', 'Model Name', 'UPC', 'Part Number', 'Color name', 'Size Name', 'Manufacturer Name', 'List Price', 'Item Count', 'Item Package Quantity', 'Product Description']\r\n",
        "    finder(labels.get(find), soup.text, look_ahead=10)\r\n",
        "except:\r\n",
        "    \"something happened\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eFvkSVc-l-Ha"
      },
      "source": [
        "#@title By id and attribute\r\n",
        "try:\r\n",
        "    id = \"ProductPrice\" #@param {type:\"string\"}\r\n",
        "    attribute = \"itemprop\" #@param {type:\"string\"}\r\n",
        "    try:\r\n",
        "        wd.find_element_by_id(id).get_attribute(attribute=attribute)\r\n",
        "    except:\r\n",
        "        print(\"something happened\")\r\n",
        "except:\r\n",
        "    \"something happened\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCxOZ4LqeBs"
      },
      "source": [
        "Most of this can be done through the libraries: soup, and BeautifulSoup4.\r\n",
        "\r\n",
        "For sites which rely heavly on JS it can also be usefull to use Phantom or headless_driver to access specific attributes.\r\n",
        "```\r\n",
        "def get_attribute(id, attribute):\r\n",
        "    return headles_driver.find_element_by_id(id).get_attribute(attribute)\r\n",
        "```\r\n",
        "These two last options are slower to load and often you can find a workaround using soup.decode.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZ-XSDN1upf"
      },
      "source": [
        "# Recursive search\r\n",
        "A different paradigm is to use a list of URL to retrive and save their soups at once, thus reducing server work and reducing processing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAg7udQZnziR"
      },
      "source": [
        "## Additional setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ruNqf-fQg9b",
        "cellView": "form"
      },
      "source": [
        "#@title Additional libraries\n",
        "\n",
        "#@markdown * Import libraries\n",
        "import json\n",
        "import time\n",
        "import unicodedata\n",
        "from fake_useragent import UserAgent\n",
        "# from w3lib.html import replace_entities\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from google.colab import data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHLaDDJ_C5SJ",
        "cellView": "form"
      },
      "source": [
        "#@title Introducing a Class: RetrivePage\n",
        "\n",
        "class RetrivePage:\n",
        "    \"This is a page class meant to retrive a page and return it's soup object\"\n",
        "    def __init__(self, url, headless=False, phantom=False):\n",
        "        __variant_extractor_lambda = lambda x: np.squeeze(re.findall(r\"=(.*)\", x))\n",
        "        self.url = url\n",
        "        self.soup = bs(requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}).text)\n",
        "        self.parsed_url = unquote(url)\n",
        "        self.headless_driver = None\n",
        "        self.phantom_driver = None\n",
        "        self.variant = __variant_extractor_lambda(self.url)\n",
        "        self.metadata = {}\n",
        "        self.variant_info = {}\n",
        "        if headless:\n",
        "            self.headless_driver = get_headless_driver()\n",
        "        if phantom:\n",
        "            self.phantom_driver = get_phantom_driver()\n",
        "    \n",
        "    def apply(self, function):\n",
        "        return function(self.page)\n",
        "    \n",
        "    # # Headless driver\n",
        "    # def get_headless_driver(self):\n",
        "    #     return headles_driver.get(self.url)\n",
        "    # def build_headless(self):\n",
        "    #     self.headless_driver = self.get_headless_driver()\n",
        "\n",
        "    # # Phantom driver\n",
        "    # def build_phantom(self):\n",
        "    #     self.phantom_driver = self.get_phantom_driver()\n",
        "    # def get_phantom_driver(self):\n",
        "    #     return phantom_driver.get(self.url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNzqzKpSDY0k",
        "cellView": "form"
      },
      "source": [
        "#@title Functions\r\n",
        "#@markdown * Retriving data\r\n",
        "#Functions\r\n",
        "def load_file(file_name, usecols):\r\n",
        "    def __read_csv(file_name, usecols=useful_cols, **kwargs):\r\n",
        "        try:\r\n",
        "            return pd.read_csv(file_name, usecols)\r\n",
        "        except:\r\n",
        "            display(\"Dismissed: useful_cols\")\r\n",
        "            return pd.read_csv(file_name)\r\n",
        "\r\n",
        "    def __read_excel(file_name, usecols=useful_cols, **kwargs):\r\n",
        "        try:\r\n",
        "            return pd.read_csv(file_name, usecols)\r\n",
        "        except:\r\n",
        "            display(\"Dismissed: useful_cols\")\r\n",
        "\r\n",
        "            return pd.read_excel(file_name)\r\n",
        "    # read\r\n",
        "    if file_name.rsplit('.',1)[-1] in ['csv','text']:\r\n",
        "        print('text')\r\n",
        "        return __read_csv(file_name, usecols=useful_cols, **kwargs)\r\n",
        "    elif file_name.rsplit('.',1)[-1] in ['xls','xlsx']:\r\n",
        "        print('Excel')\r\n",
        "        return __read_excel(file_name, usecols=useful_cols, **kwargs)\r\n",
        "    else:\r\n",
        "        \"unknown format\"\r\n",
        "#@markdown List of useful columns\r\n",
        "#[\"url\",\"brand_name\",\"manufacturer_name\",\"product_description\",\"model\",\"color_name\",\"item_package_quantity\", \"unit_count\"]\r\n",
        "all_columns=[\"title\",\"bread_crumb1\",\"bread_crumb2\",\"bread_crumb3\",\"brand_name\",\r\n",
        "             \"manufacturer_name\",\"model\",\"upc\",\"color_name\",\"size_name\",\r\n",
        "             \"item_package_quantity\",\"part_number\",\"list_price\",\"unit_count\",\r\n",
        "             \"product_description\"]\r\n",
        "useful_cols =  [\"url\",\"brand_name\",\"manufacturer_name\",\"product_description\",\"model\",\"color_name\",\"item_package_quantity\", \"unit_count\"]#@param {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tFLhrZBn2ye"
      },
      "source": [
        "## Workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaO4JwhyDnJM",
        "cellView": "form"
      },
      "source": [
        "#@title Get the Websites\r\n",
        "wait_time = 0 #@param {type:\"slider\", min:0, max:10, step:0.1}\r\n",
        "#@markdown Select **user agent**\r\n",
        "# User agent to workaround sites that don't allow bots\r\n",
        "header = \"Chrome\" #@param [\"Chrome\", \"Edge\", \"Random\", \"None\"]\r\n",
        "usr_agent = UserAgent()\r\n",
        "\r\n",
        "def user_agent_retrive(i):\r\n",
        "    switcher={\r\n",
        "            \"Edge\":usr_agent.edge,\r\n",
        "            \"Chrome\":usr_agent.chrome,\r\n",
        "            \"Random\":usr_agent.random\r\n",
        "            }\r\n",
        "    return switcher.get(i,None)\r\n",
        "header = user_agent_retrive(header)\r\n",
        "#@markdown _Looking for more options?_  [fake_useragent](https://pypi.python.org/pypi/fake-useragent)\r\n",
        "def load_URLs(URL):\r\n",
        "    r = requests.request('GET', URL, headers={'User-Agent': usr_agent})\r\n",
        "    time.sleep(wait_time)\r\n",
        "    return bs(r.text), phantom_driver.get(URL), headles_driver.get(URL)\r\n",
        "file_name = \"/content/fadlash.com_products.csv\" #@param {type:\"string\"}\r\n",
        "kwargs  =  {} #@param {type:\"raw\"}\r\n",
        "\r\n",
        "#@markdown Data loaded as: data\r\n",
        "data = load_file(file_name, usecols=useful_cols)\r\n",
        "\r\n",
        "## separating URLs\r\n",
        "url_variant = data.url[data.url.apply(lambda x: \"=\" in x)]\r\n",
        "not_variant = data.url[data.url.apply(lambda x: \"=\" not in x)]\r\n",
        "\r\n",
        "print(f\"Not variant URLs: {len(not_variant)}\\n\"\r\n",
        "      f\"URL with variant: {len(url_variant)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVgu2VkwMMoN",
        "cellView": "form"
      },
      "source": [
        "#@markdown Create a new pd.Series to store the web scrapped RetrivePage objects.<br>\r\n",
        "#@markdown ...loading\r\n",
        "start_time = time.time()\r\n",
        "stopwatch = lambda x: time.time() - start_time\r\n",
        "\r\n",
        "pages_1 = data[:40].url.apply(RetrivePage), print(\"20%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_2 = data[40:80].url.apply(RetrivePage), print(\"40%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_3 = data[80:120].url.apply(RetrivePage), print(\"60%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_4 = data[120:160].url.apply(RetrivePage), print(\"80%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages_5 = data[160:].url.apply(RetrivePage), print(\"100%\\t\", f\"time {stopwatch(start_time)}\")\r\n",
        "pages = pd.concat([pages_1[0], pages_2[0], pages_3[0],pages_4[0], pages_5[0]])\r\n",
        "print(\"Completed%\", \"time: \", stopwatch(start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaf4USSXi_v1",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### **Start** ► URL & title\r\n",
        "_get_url = lambda x:x.url\r\n",
        "urls = pages.apply(_get_url)\r\n",
        "result = {\"url\":urls} #@param {type:\"raw\"}\r\n",
        "#@markdown include titles?\r\n",
        "boolean = False #@param {'type':'boolean'}\r\n",
        "if boolean:\r\n",
        "    _get_title = lambda x:x.soup.title.text.strip('\\n').rstrip('\\n')\r\n",
        "    titles = pages.apply(_get_title)\r\n",
        "    result.update({\"title\":titles})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBt-fL60HlRl",
        "cellView": "form"
      },
      "source": [
        "#@markdown Progress\r\n",
        "show_progress = True #@param {type:\"boolean\"}\r\n",
        "df_result = pd.DataFrame(result)\r\n",
        "df_result.fillna(\"___BLANK_CELL___\", inplace=True)\r\n",
        "if show_progress:\r\n",
        "    display(data_table.DataTable(df_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s62QQGdYHLNu"
      },
      "source": [
        "df_result[0:50].replace('',fill_na_with)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "148CcoNQCk9_"
      },
      "source": [
        "#@markdown Export\r\n",
        "reindex_join_df = True  #@param {type:\"boolean\"}\r\n",
        "fill_na_with = \"___BLANK_CELL___\" #@param  {type:\"string\"}\r\n",
        "rsuffix='_reviewed' #@param {type:\"string\"}\r\n",
        "\r\n",
        "_export_name = file_name.split('.')[0] + \"_reviwed.xlsx\"\r\n",
        "_result_column_set = set(df_result.columns)\r\n",
        "_data_column_set = set(data.columns)\r\n",
        "_intersection = _data_column_set.intersection(_result_column_set)\r\n",
        "_intersection.discard('url') # inplace opperation\r\n",
        "print(\"Intersection\", _intersection)\r\n",
        "\r\n",
        "df_join = data.drop(columns=_intersection)\r\n",
        "df_join = pd.concat([df_join.set_index('url'), df_result.set_index('url')],axis=1,)\\\r\n",
        "                .fillna(fill_na_with)\r\n",
        "\r\n",
        "_df_join_columns=set(df_join.columns)\r\n",
        "_df_join_columns.discard('url')\r\n",
        "\r\n",
        "if reindex_join_df:\r\n",
        "    try:\r\n",
        "        df_join = df_join.reindex(columns=list(_df_join_columns))\r\n",
        "        print(f\"df_join \\t[reindexed]\\t{time.time()}\")\r\n",
        "    except:\r\n",
        "        print(f\"df_join \\t[couldn't reindex]\\t{time.time()}\")\r\n",
        "        display(_df_join_columns.symmetric_difference(_data_column_set))\r\n",
        "    df_join.to_excel(_export_name)\r\n",
        "else:\r\n",
        "    df_result.to_excel(_export_name)\r\n",
        "    print(f\"df_result\\t{time.time()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8tBMDOo13GO"
      },
      "source": [
        "# df_result = df_result.rename(columns={'model':barcode,\"upc\":upcs, \"notes\":size_})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zCmS9GjD08",
        "cellView": "form"
      },
      "source": [
        "#@markdown Find url' index\r\n",
        "urls_dict = {url:i for i,url in enumerate(urls)}\r\n",
        "url = \"https://fadlash.com/products/eyelash-extension-glue-sky-s?variant=32657254154319\" #@param {type:\"string\"}\r\n",
        "urls_dict.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWOTxSOT2mZn",
        "cellView": "form"
      },
      "source": [
        "#@markdown Example page: example\n",
        "index =    144#@param {type:\"integer\"}\n",
        "page = pages[index]\n",
        "soup = page.soup\n",
        "url = page.url\n",
        "variant = page.variant\n",
        "example = {'index':index,\n",
        "           'page':page,\n",
        "           'soup':soup,\n",
        "           'url':url,\n",
        "           'variant':variant}\n",
        "print(f\"url: {page.url}\")\n",
        "show = False #@param {type:\"boolean\"}\n",
        "if show:\n",
        "    render(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83C_nC6vLIT"
      },
      "source": [
        "## Finding additional attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLOGTKeRHafO"
      },
      "source": [
        "#### From scripts\r\n",
        "Often pages relay on a data structure for keeping the information of the different variants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9bKWDdBzrfG",
        "cellView": "form"
      },
      "source": [
        "#@markdown For when the metadata is not in a regular variable\r\n",
        "identifier=\"OptionSelectors('productSelect', {\\n      product: \"\r\n",
        "\r\n",
        "def __get_metadata(page):\r\n",
        "    script = [str(i) for i in soup.find_all('script') if identifier in str(i)][0]\r\n",
        "    script = script.split(identifier)[1]\r\n",
        "    script = script.split(\"onVariantSelected:\")[0].rstrip('\\n,\\n      ')\r\n",
        "    metadata = json.loads(script)\r\n",
        "    page.metadata = metadata\r\n",
        "    # return colors \r\n",
        "\r\n",
        "metadata = pages.apply(__get_metadata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eqNZml4NnIr"
      },
      "source": [
        "# @title Metadata\r\n",
        "# @markdown This is useful when you have a tag such as **var meta =**\r\n",
        "var_name = \"var meta = \" #@param {'type':'string'}\r\n",
        "key_name = \"meta\" #@param {'type':'string'}\r\n",
        "if key_name is None:\r\n",
        "    key_name = var_name.strip().split()[1]\r\n",
        "get_variant_info = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "print(f\"key_name: {key_name}\")\r\n",
        "def __get_variant_info(page):\r\n",
        "    # try:\r\n",
        "    #     variants = page.metadata['product']['variants']\r\n",
        "    #     return [i for i in variants if int(i['id']) == int(page.variant)][0]\r\n",
        "    # except: None\r\n",
        "    try: return [v for v in page.metadata[key_name]['product']['variants'] if int(v['id']) == int(page.variant)][0]\r\n",
        "    except: None\r\n",
        "\r\n",
        "def get_meta(page):\r\n",
        "    soup = page.soup\r\n",
        "    scripts = soup.find_all('script')\r\n",
        "    # mets = ''\r\n",
        "    for s in scripts:\r\n",
        "        if var_name in s.text:          # find the script of interest\r\n",
        "            # Exctract the json value of var_name\r\n",
        "            meta = s.text\r\n",
        "            meta = meta.split(var_name)[1].split('};')[0]+'}'\r\n",
        "            # Load the json and make's sure the format is correct\r\n",
        "            json_meta = json.loads(meta)\r\n",
        "            page.metadata.update({key_name:json_meta}) # add metadata to the RetrivedPage\r\n",
        "            if get_variant_info:\r\n",
        "                page.variant_info.update({key_name:__get_variant_info(page)})\r\n",
        "            return json_meta\r\n",
        "json_meta = pages.apply(get_meta)\r\n",
        "# @markdown **json_meta*: results\r\n",
        "# result_key = \"metadata\" #@param {type:\"string\"}\r\n",
        "# result.update({result_key:metadata}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62FjjW8QwJPy"
      },
      "source": [
        "#@markdown Variant attributes\r\n",
        "def _extract_options(page):\r\n",
        "    try:\r\n",
        "        return {\r\n",
        "            \"name\":page.variant_info['meta']['name'],\r\n",
        "            # \"barcode\":page.variant_info['SUBParams']['barcode'],\r\n",
        "            \"sku\":page.variant_info['meta']['sku'],\r\n",
        "            'public_title':page.variant_info['meta']['public_title']\r\n",
        "            }\r\n",
        "    except: return {\"name\":None,\"barcode\": None,\"sku\":None, \"public_title\":None}\r\n",
        "ids = pages.apply(_extract_options)\r\n",
        "\r\n",
        "names = ids.apply(lambda x: x['name'])\r\n",
        "# barcode = ids.apply(lambda x: x['barcode'])\r\n",
        "upcs = ids.apply(lambda x: x['sku'])\r\n",
        "sizes = ids.apply(lambda x: x['public_title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-5jYJc1JaF6",
        "cellView": "form"
      },
      "source": [
        "#@markdown experiments\r\n",
        "def __get_options(page):\r\n",
        "    try: \r\n",
        "        options = [i for i in page.variant_info['name'].split() if i != '-'][:2]\r\n",
        "    except:\r\n",
        "        options = [None,None]\r\n",
        "    return {\"color\":options[0], \"size\":options[1]}\r\n",
        "options = pages.apply(__get_options)\r\n",
        "size_ = options.apply(lambda x: x['size'])\r\n",
        "color = options.apply(lambda x: x['color'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAMmoPmfBF7B"
      },
      "source": [
        "result.update({\"name\":names,\r\n",
        "               \"upc\":upcs})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dT1edfOzh2H"
      },
      "source": [
        "### Done:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dry_cvNmHErA"
      },
      "source": [
        "result.update({\"title\":names,\r\n",
        "               \"bread_crumb1\":bread_crumb1,\r\n",
        "               \"bread_crumb2\":bread_crumb2,\r\n",
        "               \"bread_crumb3\":bread_crumb3,\r\n",
        "            #    \"product_description\":descriptions,\r\n",
        "            #    \"color_name\":colors,\r\n",
        "            \"size_\":size_,\r\n",
        "               \"size_name\":sizes,\r\n",
        "               \"upc\":upcs,\r\n",
        "               \"barcode\":barcode})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9rYG6dwvT4L"
      },
      "source": [
        "#@markdown Title\r\n",
        "def __get_titles(page):\r\n",
        "    try:\r\n",
        "        page.title = page.soup.h1.text\r\n",
        "        return page.soup.h1.text\r\n",
        "    except: return None\r\n",
        "\r\n",
        "titles=pages.apply(__get_titles)\r\n",
        "titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rokn8GwLyWnY"
      },
      "source": [
        "#@markdown Crumbs\r\n",
        "def __get_crumbs(page):\r\n",
        "    try: \r\n",
        "        crumbs = page.soup\\\r\n",
        "                    .find(\"div\", {'id':\"breadcrumb\",\r\n",
        "                                  \"class\":\"desktop-12 tablet-6 mobile-3\"})\\\r\n",
        "                    .text.strip().split('\\n»\\n')\r\n",
        "    except:\r\n",
        "        return (None, None, None)\r\n",
        "    try:\r\n",
        "        if page.title != crumbs[2]:\r\n",
        "            return (crumbs[0], crumbs[1], crumbs[2])\r\n",
        "        else:\r\n",
        "            return (crumbs[0], crumbs[1], None)\r\n",
        "    except:\r\n",
        "        try:\r\n",
        "            return (crumbs[0], crumbs[1], None)\r\n",
        "        except:\r\n",
        "            return (None, None, None)\r\n",
        "bread_crumbs = pages.apply(__get_crumbs)\r\n",
        "bread_crumb1 = crumbs.apply(lambda x: x[0])\r\n",
        "bread_crumb2 = crumbs.apply(lambda x: x[1])\r\n",
        "bread_crumb3 = crumbs.apply(lambda x: x[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyLgZlQ72vFZ"
      },
      "source": [
        "#@markdown Descriptions\r\n",
        "def __get_descriptions(page):\r\n",
        "    try: return unicodedata.normalize(\"NFKC\", page.soup\\\r\n",
        "                    .find(\"div\", {\"class\":\"product-description rte\",\r\n",
        "                                \"itemprop\":\"description\"})\\\r\n",
        "                    .text.strip('\\n'))\r\n",
        "    except: return None\r\n",
        "descriptions = pages.apply(__get_descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWuejkWlpmBY"
      },
      "source": [
        "#@markdown UPC\r\n",
        "# pattern=re.compile('[\\w\\d-]+')\r\n",
        "\r\n",
        "def __get_upc(page):\r\n",
        "    try: return page.variant_info['sku']\r\n",
        "    except: return None\r\n",
        "\r\n",
        "upcs = pages.apply(__get_upc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsMdEwB4TJvu"
      },
      "source": [
        "### Work In Progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dQIreuWzqSz"
      },
      "source": [
        "#### Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIxYa6vEskXM"
      },
      "source": [
        "variant_info = pages.apply(__variant_info)\r\n",
        "def __get_barcode(b):\r\n",
        "    try: return str(b['barcode'])\r\n",
        "    except: return b\r\n",
        "barcodes = barcode.apply(__get_barcode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YJzGPJdxVvC2"
      },
      "source": [
        "#@title Description\r\n",
        "def __get_descriptions(page):\r\n",
        "    soup = page.soup\r\n",
        "    content = soup.find_all('div', {\"class\":\"TabbedPanelsContent\"})\r\n",
        "    description_tabs = soup.find(\"ul\", attrs={\"class\":\"TabbedPanelsTabGroup\"})\r\n",
        "    description_tabs = description_tabs.text\\\r\n",
        "                                    .rstrip('\\n').strip('\\n')\\\r\n",
        "                                    .split('\\n')\r\n",
        "\r\n",
        "    description = ''\r\n",
        "    ignore = ['Optional Accessories', 'Impeller Kits']\r\n",
        "\r\n",
        "    def clean_text(text_to_clean):\r\n",
        "        return text_to_clean.strip('\\n').rstrip('\\n').strip('\\xa0').rstrip('\\n').strip('\\n')\r\n",
        "\r\n",
        "    def __post_process_description(description):\r\n",
        "        description_noWarranty = ''\r\n",
        "        for i, strr in enumerate(description.split('\\n')):\r\n",
        "            if (len(strr) < 15) & (\"arranty\" in strr):\r\n",
        "                ++i\r\n",
        "            else:\r\n",
        "                description_noWarranty += strr\r\n",
        "        return description_noWarranty\\\r\n",
        "                            .rstrip('\\n')\\\r\n",
        "                            .replace('â\\x80¢', '•')\\\r\n",
        "                            .replace('Â°', '°')\\\r\n",
        "                            .replace('âs',\"'s\")\r\n",
        "                            \r\n",
        "\r\n",
        "    try:\r\n",
        "        for i, tab_name in enumerate(description_tabs):\r\n",
        "            if tab_name not in ignore:\r\n",
        "                info = clean_text(content[i].text)\r\n",
        "                description += tab_name +  '\\n' + info + '\\n'\r\n",
        "        return __post_process_description(description)\r\n",
        "    except IndexError:\r\n",
        "        print(page.url)\r\n",
        "descriptions = pages.apply(__get_descriptions)\r\n",
        "# df = df.join(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzWrU5G0Yyoi"
      },
      "source": [
        "### json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn2tytkg_FVZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Find Attributes\n",
        "dc = json.loads(finder(r'(?:var meta = )(.*)(?:};)', s.text)[0].replace('var meta = ',''))\n",
        "try:\n",
        "    variant_num = URL.split('variant=')[1]\n",
        "    variant = [i for i in dc['product']['variants'] if i['id']==int(variant_num)][0]\n",
        "except IndexError:\n",
        "    variant = dc['product']['variants'][0]\n",
        "#@markdown * title\n",
        "title = driver.find_element_by_class_name(\"standard-single\").text\n",
        "\n",
        "#@markdown * Public_title  | _has the size values_\n",
        "public_title = variant['public_title'] \n",
        "\n",
        "#@markdown * brand\n",
        "brand = finder(r\"(?:\\\"brand\\\")(.)\", s.text, look_ahead=20)[1].split(\":\")[1].replace('\"','')\n",
        "\n",
        "#@markdown * manofacturer\n",
        "try:\n",
        "    label = dc['product']['label']\n",
        "except:\n",
        "    label = finder(r\"(?:\\\"label\\\")(.)\", s.text, look_ahead=20)[1].split(\":\")[1].replace('\"','')\n",
        "\n",
        "#@markdown * Price\n",
        "price = wd.find_element_by_id('ProductPrice')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZvKuWVpjZ--"
      },
      "source": [
        "# Crawler\r\n",
        "Finally, another paradigm is defining a the page structure to obtain the values using a crawler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cJnl98h-ZB2S"
      },
      "source": [
        "#@title Imports\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRfLRfKpQZhF"
      },
      "source": [
        "# Key- Retrival function\r\n",
        "kwargs = {'title':(lambda p: p.title)}\r\n",
        "\r\n",
        "# Class: Website \r\n",
        "class Website:\r\n",
        "    \"\"\"\r\n",
        "    Contains information about website structure.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, url, **kwargs):\r\n",
        "        self.url = url\r\n",
        "        self.__dict__.update(kwargs)\r\n",
        "\r\n",
        "# Initialize the website as an empty entity\r\n",
        "# kwargs_n = {keys:None for keys in kwargs.keys()}\r\n",
        "# w = Website(page.url, kwargs_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cz56pzXjY9G"
      },
      "source": [
        "class Crawler:\r\n",
        "    def __init__(self, attrs:[dir]):\r\n",
        "        self.attrs=attrs\r\n",
        "\r\n",
        "    def getPage(self, url):\r\n",
        "        try:\r\n",
        "            req = requests.get(url)\r\n",
        "        except requests.exceptions.RequestException:\r\n",
        "            return None\r\n",
        "        return BeautifulSoup(req.text, 'html.parser')\r\n",
        "\r\n",
        "    def safeGet(self, pageObj, selector):\r\n",
        "        \"\"\"\r\n",
        "        Utility function used to get a content string from a\r\n",
        "        Beautiful Soup object and a selector. Returns an empty\r\n",
        "        string if no object is found for the given selector\r\n",
        "        \"\"\"\r\n",
        "        selectedElems = pageObj.soup.select(selector)\r\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\r\n",
        "            return '\\n'.join([elem.get_text()\r\n",
        "                for elem in selectedElems])\r\n",
        "        return 'empty'\r\n",
        "\r\n",
        "    def ifer(self, page, sfun:[str, callable]):\r\n",
        "        if callable(sfun):\r\n",
        "            return sfun(page)\r\n",
        "        else:\r\n",
        "            return self.safeGet(page, sfun)\r\n",
        "\r\n",
        "    def parser(self, pageObj=None, url:[str]=None):\r\n",
        "        \"\"\"\r\n",
        "        Extract content from a given page URL\r\n",
        "        \"\"\"\r\n",
        "        if pageObj is None:\r\n",
        "            if url is not None:\r\n",
        "                pageObj = self.getPage(url)\r\n",
        "            else:\r\n",
        "                \"You need to pass one of pageObj/url\"\r\n",
        "        else:\r\n",
        "            url = pageObj.url\r\n",
        "\r\n",
        "        attrs = {key:self.ifer(pageObj, sfun)\r\n",
        "                for key, sfun in self.attrs.items()\r\n",
        "                }\r\n",
        "        return Website(pageObj.url, page=pageObj, **attrs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrqXWzoV3VQ5"
      },
      "source": [
        "# c = Crawler(kwargs)\r\n",
        "w=c.parser(page.soup, page.url)\r\n",
        "page.url"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}